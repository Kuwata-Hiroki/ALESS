# -*- coding: utf-8 -*-
"""Resnet for plant image classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pyJO8d3RWyOHp932SiaAWRplgj__T7_l
"""

#@title ドライブのマウント & 必要なライブラリのインポート
from google.colab import drive
drive.mount('/content/drive')


import math
import random
import os
import pandas as pd
from PIL import Image
import cv2
from IPython.display import display, HTML
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from sklearn.model_selection import train_test_split
from torchvision.models import resnet50, ResNet50_Weights, resnet34, ResNet34_Weights
import torchvision.models as models
from sklearn.model_selection import ParameterGrid
from sklearn.model_selection import ParameterSampler
!pip install scikit-optimize
from skopt import gp_minimize
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer
np.random.seed(314)

#@title 準備

#GPUが使えるなら指定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#データ学習時の前処理を指定
class CustomImageDataset(Dataset):
    def __init__(self, dataframe, img_dir, transform=None, target_transform=None):
        self.dataframe = dataframe
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        image = self.dataframe.iloc[idx, 0]
        label = int(self.dataframe.iloc[idx, 1])

        if self.transform:
            image = self.transform(image)

        if self.target_transform:
            label = self.target_transform(label)

        image = torch.reshape(image, (3, 256, 256))

        return image, label

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
])

df = pd.DataFrame(np.load('/content/drive/MyDrive/aless/dataframe.npy', allow_pickle=True))
for i in range(len(df)):
  df.iloc[i,0] = Image.fromarray(df.loc[i,0])

#@title ハイパーパラメータ(grid)
num_epochs = 10
test_size = 48 #validationに使う写真の枚数
short_log = True #Stepごとの出力を消す

#バッチ数はlayer34のとき128以下、layer50のとき48以下にすること。
param_grid = {
    'batch_size': [16,32,48,64],
    'learning_rate': [0.1,0.01,0.001,0.0001],
    'optimizer':[optim.Adam,optim.SGD,optim.RMSprop],
    'pretrained':[False],
    'layer_num': [34]
}

#@title 学習&プロット

#テストデータ分割
df_train, df_val = train_test_split(df, test_size=test_size, random_state=42)

img_dir = "/content/drive/MyDrive/aless"
training_data = CustomImageDataset(dataframe=df_train, img_dir=img_dir, transform=transform)
test_data = CustomImageDataset(dataframe=df_val, img_dir=img_dir, transform=transform)

#for params in ParameterGrid(param_grid):
for params in ParameterGrid(param_grid):
    print("batch_size:",params['batch_size'],"learning_rate:",params['learning_rate'])
    print("optimizer:",params['optimizer'],"pretrained:",params['pretrained'])

    #損失関数と最適化手法を指定
    if (params['layer_num'] == 34):
      if params['pretrained']:
        weights = ResNet34_Weights.DEFAULT
        model = resnet34(weights=weights)
      else:
        model = resnet34()
    else:
      if params['pretrained']:
        weights = ResNet50_Weights.DEFAULT
        model = resnet50(weights=weights)
      else:
        model = resnet50()

    loss_fn = nn.CrossEntropyLoss()
    optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])

    #バッチサイズ分だけデータを取ってくる
    train_dataloader = DataLoader(training_data, batch_size=params['batch_size'], shuffle=True, drop_last=True)
    test_dataloader = DataLoader(test_data, batch_size=len(test_data))

    total_step = len(train_dataloader)
    loss_list = []
    acc_list = []

    model = model.to(device)

    val_loss_list = []
    val_acc_list = []

    A = str(params['optimizer'])+ "/"
    B = str(params['batch_size'])+"_"+str(params['learning_rate'])

    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(train_dataloader):

            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = loss_fn(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total = labels.size(0)
            _, predicted = torch.max(outputs.data, 1)
            correct = (predicted == labels).sum().item()
            acc_list.append((correct / total)*100)

            loss_list.append(loss.item())

            for j, (val_images, val_labels) in enumerate(test_dataloader):
                val_images = val_images.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_images)
                _, val_predicted = torch.max(val_outputs.data, 1)
                val_total = val_labels.size(0)
                val_correct = (val_predicted == val_labels).sum().item()

                val_loss = loss_fn(val_outputs, val_labels)
                val_loss_list.append(val_loss.item())

                val_accuracy = val_correct / val_total * 100
                val_acc_list.append(val_accuracy)

            if ((not(short_log)) or (i+1 == total_step)):
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))
                print('Test Accuracy: {} %'.format(val_accuracy))

    #プロット
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(loss_list) + 1), loss_list, label='Training Loss')
    plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='Validation Loss')
    plt.legend()
    plt.grid()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')

    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(acc_list) + 1), acc_list, label='Training Accuracy')
    plt.plot(range(1, len(val_acc_list) + 1), val_acc_list, label='Validation Accuracy')
    plt.legend()
    plt.grid()
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    drive_save_dir="/content/drive/My Drive/colab_graph"
    plt.savefig(drive_save_dir+"/"+A+B+".png")
    plt.show()

#@title 学習&プロット(random)

for i in range(15):
    n = np.random.seed()
    a = np.random.randint(1,8)
    b = np.random.rand()
    #c = np.random.randint(1,3)
    c = 3
    if c == 1:
        param_grid = {
        'batch_size': [8*a],
        'learning_rate': [pow(0.1,(b*3+1))],
        'optimizer': [optim.Adam],
        'pretrained':[True],
        'layer_num': [34]
        }
    if c == 2:
        param_grid = {
        'batch_size': [8*a],
        'learning_rate': [pow(0.1,(b*3+1))],
        'optimizer': [optim.SGD],
        'pretrained':[True],
        'layer_num': [34]
        }
    if c == 3:
        param_grid = {
        'batch_size': [8*a],
        'learning_rate': [pow(0.1,(b*3+1))],
        'optimizer': [optim.RMSprop],
        'pretrained':[True],
        'layer_num': [34]
        }
    df_train, df_val = train_test_split(df, test_size=test_size, random_state=42)

    img_dir = "/content/drive/MyDrive/aless"
    training_data = CustomImageDataset(dataframe=df_train, img_dir=img_dir, transform=transform)
    test_data = CustomImageDataset(dataframe=df_val, img_dir=img_dir, transform=transform)

    #for params in ParameterGrid(param_grid):
    for params in ParameterGrid(param_grid):
        print("batch_size:",params['batch_size'],"learning_rate:",params['learning_rate'])
        print("optimizer:",params['optimizer'],"pretrained:",params['pretrained'])

    #損失関数と最適化手法を指定
        if (params['layer_num'] == 34):
          if params['pretrained']:
              weights = ResNet34_Weights.DEFAULT
              model = resnet34(weights=weights)
          else:
              model = resnet34()
        else:
          if params['pretrained']:
              weights = ResNet50_Weights.DEFAULT
              model = resnet50(weights=weights)
          else:
              model = resnet50()

        loss_fn = nn.CrossEntropyLoss()
        optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])

    #バッチサイズ分だけデータを取ってくる
        train_dataloader = DataLoader(training_data, batch_size=params['batch_size'], shuffle=True, drop_last=True)
        test_dataloader = DataLoader(test_data, batch_size=len(test_data))

        total_step = len(train_dataloader)
        loss_list = []
        acc_list = []

        model = model.to(device)

        val_loss_list = []
        val_acc_list = []

        A = str(params['optimizer'])+ "/"
        B = str(params['batch_size'])+"_"+str(params['learning_rate'])

        for epoch in range(num_epochs):
            for i, (images, labels) in enumerate(train_dataloader):

                images = images.to(device)
                labels = labels.to(device)

                outputs = model(images)
                loss = loss_fn(outputs, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total = labels.size(0)
                _, predicted = torch.max(outputs.data, 1)
                correct = (predicted == labels).sum().item()
                acc_list.append((correct / total)*100)

                loss_list.append(loss.item())

                for j, (val_images, val_labels) in enumerate(test_dataloader):
                    val_images = val_images.to(device)
                    val_labels = val_labels.to(device)
                    val_outputs = model(val_images)
                    _, val_predicted = torch.max(val_outputs.data, 1)
                    val_total = val_labels.size(0)
                    val_correct = (val_predicted == val_labels).sum().item()

                    val_loss = loss_fn(val_outputs, val_labels)
                    val_loss_list.append(val_loss.item())

                    val_accuracy = val_correct / val_total * 100
                    val_acc_list.append(val_accuracy)

                if ((not(short_log)) or (i+1 == total_step)):
                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))
                    print('Test Accuracy: {} %'.format(val_accuracy))

    #プロット
        plt.figure(figsize=(12, 6))

        plt.subplot(1, 2, 1)
        plt.plot(range(1, len(loss_list) + 1), loss_list, label='Training Loss')
        plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='Validation Loss')
        plt.legend()
        plt.grid()
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')

        plt.subplot(1, 2, 2)
        plt.plot(range(1, len(acc_list) + 1), acc_list, label='Training Accuracy')
        plt.plot(range(1, len(val_acc_list) + 1), val_acc_list, label='Validation Accuracy')
        plt.legend()
        plt.grid()
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy')
        drive_save_dir="/content/drive/My Drive/colab_graph_random"
        plt.savefig(drive_save_dir+"/"+A+B+".png")
        plt.show()

#@title 学習（ベイズ）
num_epochs = 10
test_size = 48 #validationに使う写真の枚数
short_log = True #Stepごとの出力を消す

param_grid = {
    'pretrained':[True],
    'layer_num': [34],
    'optimizer':[optim.Adam,optim.SGD,optim.RMSprop]

}
def func(param):
  df_train, df_val = train_test_split(df, test_size=test_size, random_state=42)

  img_dir = "/content/drive/MyDrive/aless"
  training_data = CustomImageDataset(dataframe=df_train, img_dir=img_dir, transform=transform)
  test_data = CustomImageDataset(dataframe=df_val, img_dir=img_dir, transform=transform)
  for params in ParameterGrid(param_grid):
      output = 0
      count = 0
      print("batch_size:",param[1],"learning_rate:",param[0])
      print("optimizer:",params['optimizer'],"pretrained:",params['pretrained'])

  #損失関数と最適化手法を指定
      if (params['layer_num'] == 34):
        if params['pretrained']:
            weights = ResNet34_Weights.DEFAULT
            model = resnet34(weights=weights)
        else:
            model = resnet34()
      else:
        if params['pretrained']:
            weights = ResNet50_Weights.DEFAULT
            model = resnet50(weights=weights)
        else:
            model = resnet50()

      loss_fn = nn.CrossEntropyLoss()
      optimizer = params['optimizer'](model.parameters(), lr=param[0])

  #バッチサイズ分だけデータを取ってくる
      train_dataloader = DataLoader(training_data, batch_size=round(param[1]), shuffle=True, drop_last=True)
      test_dataloader = DataLoader(test_data, batch_size=len(test_data))

      total_step = len(train_dataloader)
      loss_list = []
      acc_list = []

      model = model.to(device)

      val_loss_list = []
      val_acc_list = []

      A = str(params['optimizer'])+ "/"
      B = str(param[1])+"_"+str(param[0])

      for epoch in range(num_epochs):
          for i, (images, labels) in enumerate(train_dataloader):

              images = images.to(device)
              labels = labels.to(device)

              outputs = model(images)
              loss = loss_fn(outputs, labels)

              optimizer.zero_grad()
              loss.backward()
              optimizer.step()

              total = labels.size(0)
              _, predicted = torch.max(outputs.data, 1)
              correct = (predicted == labels).sum().item()
              acc_list.append((correct / total)*100)

              loss_list.append(loss.item())

              for j, (val_images, val_labels) in enumerate(test_dataloader):
                  val_images = val_images.to(device)
                  val_labels = val_labels.to(device)
                  val_outputs = model(val_images)
                  _, val_predicted = torch.max(val_outputs.data, 1)
                  val_total = val_labels.size(0)
                  val_correct = (val_predicted == val_labels).sum().item()

                  val_loss = loss_fn(val_outputs, val_labels)
                  val_loss_list.append(val_loss.item())

                  val_accuracy = val_correct / val_total * 100
                  val_acc_list.append(val_accuracy)

              if ((not(short_log)) or (i+1 == total_step)):
                  print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))
                  print('Test Accuracy: {} %'.format(val_accuracy))
              if (epoch == num_epochs - 1):
                  output = output + val_accuracy
                  count = count + 1
  #プロット
      plt.figure(figsize=(12, 6))

      plt.subplot(1, 2, 1)
      plt.plot(range(1, len(loss_list) + 1), loss_list, label='Training Loss')
      plt.plot(range(1, len(val_loss_list) + 1), val_loss_list, label='Validation Loss')
      plt.legend()
      plt.grid()
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.title('Training and Validation Loss')

      plt.subplot(1, 2, 2)
      plt.plot(range(1, len(acc_list) + 1), acc_list, label='Training Accuracy')
      plt.plot(range(1, len(val_acc_list) + 1), val_acc_list, label='Validation Accuracy')
      plt.legend()
      plt.grid()
      plt.xlabel('Epoch')
      plt.ylabel('Accuracy')
      plt.title('Training and Validation Accuracy')
      drive_save_dir="/content/drive/My Drive/colab_graph_Bayesian2"
      plt.savefig(drive_save_dir+"/"+A+B+".png")
      plt.show()
      return 100-(output/count)

x1 = (0, 0.01)
x2 = (8,64)
x =  (x1, x2)
result = gp_minimize(func, x,
                        n_calls=16,
                        noise=0.0,
                        model_queue_size=1,
                        verbose=True)
print(result.fun)
print(result.x)

#@title 写真処理（写真を新たに追加した場合のみ実行）
#すべての写真を数値表現にする
path_base = "/content/drive/MyDrive/aless"
plant_names = ["カラムシ", "ツバキ", "ツツジ", "ドクダミ"]
label = 0

dfs_by_plant = {}

for plant_name in plant_names:
    folder_path = os.path.join(path_base, plant_name)

    file_list = os.listdir(folder_path)
    print(plant_name, len(file_list))
    image_data = []

    for file_name in file_list:
        file_path = os.path.join(folder_path, file_name)

        if file_name.lower().endswith('.jpg'):
            img_path = os.path.join(path_base, plant_name, file_name)
            image = Image.open(img_path) #.convert("RGB")はいらないかも
            #image = Image.fromarray(cv2.resize(np.array(image, dtype=np.uint8), (256, 256))) #PIL ->OpenCV -> PIL
            image = cv2.resize(np.array(image, dtype=np.uint8), (256, 256))
            image_data.append({'image_data': image})

    df = pd.DataFrame(image_data)

    df['label'] = label
    label += 1

    dfs_by_plant[plant_name] = df

df = pd.concat(dfs_by_plant.values(), ignore_index=True)

np.save('/content/drive/MyDrive/aless/dataframe', df.to_numpy())